# PolyOpt - Polyhedral Loop Optimizer

<p align="center">
  <strong>A research-grade polyhedral compiler for optimizing loop nests</strong>
</p>

<p align="center">
  <a href="#features">Features</a> •
  <a href="#installation">Installation</a> •
  <a href="#quick-start">Quick Start</a> •
  <a href="#examples">Examples</a> •
  <a href="#commands">Commands</a> •
  <a href="#the-polyhedral-model">Theory</a>
</p>

---

## Overview

PolyOpt is a polyhedral compiler that automatically optimizes loop nests for:
- **Parallelism**: Identify and exploit parallel loops
- **Data Locality**: Improve cache utilization through tiling
- **Vectorization**: Enable SIMD optimizations

It takes programs written in a simple `.poly` language and generates optimized C code with OpenMP annotations.

```
┌─────────────┐     ┌──────────────┐     ┌────────────────┐     ┌─────────────┐
│  .poly file │ ──► │  Parse & IR  │ ──► │  Dependence    │ ──► │  Transform  │
│             │     │  Lowering    │     │  Analysis      │     │  & Schedule │
└─────────────┘     └──────────────┘     └────────────────┘     └──────┬──────┘
                                                                       │
                                                                       ▼
                                                               ┌─────────────┐
                                                               │  Generate   │
                                                               │  C/OpenMP   │
                                                               └─────────────┘
```

## Features

- **Frontend**: Custom parser for a simple loop-focused language
- **Analysis**: 
  - Dependence analysis (GCD test, Banerjee test)
  - Parallelism detection
  - Loop-carried vs loop-independent dependencies
- **Transformations**:
  - Loop tiling (blocking) for cache optimization
  - Loop interchange for better memory access
  - Loop fusion and distribution
  - Automatic scheduling (Pluto, Feautrier, Greedy algorithms)
- **Code Generation**:
  - Clean C output
  - OpenMP parallelization pragmas
  - Vectorization hints
  - Benchmark code generation

---

## Installation

### Prerequisites

- Rust 1.70+ (install from [rustup.rs](https://rustup.rs))
- GCC or Clang (for compiling generated code)

### Build from Source

```bash
# Clone the repository
git clone https://github.com/yourusername/polyopt.git
cd polyopt

# Build in release mode
cargo build --release

# Run tests
cargo test

# Install (optional)
cargo install --path .
```

### Verify Installation

```bash
# Check version
cargo run --bin polyopt -- --version

# See help
cargo run --bin polyopt -- --help
```

---

## Quick Start

### 1. Your First Program

Create a file `hello.poly`:

```c
// Vector addition: C[i] = A[i] + B[i]
func vector_add(A[N], B[N], C[N]) {
    for i = 0 to N {
        C[i] = A[i] + B[i];
    }
}
```

### 2. Compile to C

```bash
cargo run --bin polyopt -- compile hello.poly
```

Output:
```c
// Generated by PolyOpt - Polyhedral Optimizer

#include <stdio.h>
#include <stdlib.h>
...

void vector_add(int N, double* A, double* B, double* C) {
    // Statement S0: S0
    for (int i = 0; i < N; i++) {
        C[i] = (A[i] + B[i]);
    }
}
```

### 3. Add OpenMP Parallelization

```bash
cargo run --bin polyopt -- compile hello.poly --openmp
```

### 4. Analyze Dependencies

```bash
cargo run --bin polyopt -- analyze hello.poly --deps --parallel
```

### 5. Visualize Iteration Space

```bash
cargo run --bin polyvis -- hello.poly --params N=8
```

---

## The .poly Language

### Basic Syntax

```c
// Comments use // or /* */

func function_name(ArrayA[N][M], ArrayB[K], scalar_param) {
    // For loops
    for i = start to end {
        // Statements
        ArrayA[i][j] = expression;
    }
    
    // For loops with step
    for i = 0 to N step 2 {
        ...
    }
}
```

### Array Declarations

```c
// 1D array with symbolic size N
A[N]

// 2D array with sizes N x M  
B[N][M]

// 3D array
C[N][M][K]
```

### Expressions

```c
// Arithmetic
A[i] + B[j]
A[i] * 2.5
A[i] / B[j]
A[i] - B[j]

// Array access with affine indices
A[i]
B[i][j]
C[i+1][j-1]
C[2*i][j]

// Constants
0
1.5
3.14159
```

### Loop Bounds

```c
// Simple bounds
for i = 0 to N { ... }

// Offset bounds
for i = 1 to N-1 { ... }

// Triangular bounds (planned)
for j = 0 to i { ... }
```

---

## Commands

### `polyopt compile` - Generate Code

Compile a `.poly` file to C code.

```bash
# Basic compilation
polyopt compile input.poly

# Output to file
polyopt compile input.poly -o output.c

# With OpenMP
polyopt compile input.poly --openmp

# With vectorization hints
polyopt compile input.poly --openmp --vectorize

# Generate benchmark wrapper
polyopt compile input.poly --benchmark
```

**Options:**
| Option | Description |
|--------|-------------|
| `-o, --output` | Output file (default: stdout) |
| `-t, --target` | Target: `c`, `openmp`, `cuda`, `opencl` |
| `--openmp` | Enable OpenMP parallelization |
| `--vectorize` | Add SIMD pragmas |
| `--benchmark` | Generate timing code |

---

### `polyopt analyze` - Dependence Analysis

Analyze dependencies and parallelism opportunities.

```bash
# Basic analysis
polyopt analyze input.poly

# Show dependencies
polyopt analyze input.poly --deps

# Show parallelism
polyopt analyze input.poly --parallel

# Full analysis
polyopt analyze input.poly --deps --parallel --stats

# JSON output
polyopt analyze input.poly --json
```

**Example Output:**
```
=== Analysis: matmul ===

Statements: 2
Parameters: ["N", "K", "M"]
Arrays: ["A", "C", "B"]

--- Dependences ---
Total: 6
  Flow (RAW): 2
  Anti (WAR): 2
  Output (WAW): 2
  Loop-carried: 6
  Loop-independent: 0

--- Parallelism ---
  Level 0: ✗ sequential
  Level 1: ✗ sequential
  Level 2: ✗ sequential

No directly parallelizable loops found
Consider applying transformations (tiling, skewing)
```

---

### `polyopt optimize` - Apply Transformations

Apply optimizations and generate code.

```bash
# Auto-optimize with Pluto scheduler
polyopt optimize input.poly --schedule pluto

# Apply tiling
polyopt optimize input.poly --tile 32

# Combined optimizations
polyopt optimize input.poly --schedule pluto --tile 32 --openmp

# Output to file
polyopt optimize input.poly --tile 32 -o optimized.c
```

**Options:**
| Option | Description |
|--------|-------------|
| `--tile SIZE` | Apply tiling with given block size |
| `--schedule ALG` | Scheduling: `pluto`, `feautrier`, `greedy` |
| `--openmp` | Enable OpenMP in output |
| `-o, --output` | Output file |

---

### `polyopt parse` - Debug Parser

View the parsed representation of a program.

```bash
# Show AST
polyopt parse input.poly

# Show HIR (High-level IR)
polyopt parse input.poly --hir

# Show PIR (Polyhedral IR)
polyopt parse input.poly --pir
```

---

### `polyopt bench` - Run Benchmarks

Compile and run benchmarks.

```bash
# Run with default size (N=1000)
polyopt bench input.poly

# Specify size
polyopt bench input.poly -N 2000

# Multiple iterations
polyopt bench input.poly -N 1000 --iterations 10

# With OpenMP
polyopt bench input.poly -N 1000 --openmp
```

---

### `polyvis` - Visualize Iteration Spaces

Text-based visualization of iteration spaces and dependencies.

```bash
# Basic visualization
polyvis input.poly

# With parameter values
polyvis input.poly --params N=10,M=10

# Limit displayed iterations
polyvis input.poly --params N=20 --max-iters 100

# Verbose mode (show dependency details)
polyvis input.poly --params N=8 --verbose
```

**Example Output:**
```
╔══════════════════════════════════════════════════════════════════╗
║                    PolyOpt Iteration Space Visualizer            ║
╚══════════════════════════════════════════════════════════════════╝

Program: matmul
Parameters: N=8, K=8, M=8

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Statement S0 (2D domain):
  Iteration space (8 x 8 = 64 points):

    j→  0 1 2 3 4 5 6 7
  i↓
   0    ● ● ● ● ● ● ● ●
   1    ● ● ● ● ● ● ● ●
   2    ● ● ● ● ● ● ● ●
   ...

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Dependence Summary:
  Flow (RAW):   2
  Anti (WAR):   2
  Output (WAW): 2

Parallelism: No parallel loops detected at any level
```

---

## Examples

### Basic Examples

#### Vector Addition (Parallel)
```bash
# File: examples/basic/vector_add.poly
polyopt analyze examples/basic/vector_add.poly --parallel
# Output: Level 0: ✓ PARALLEL
```

#### Array Copy (2D Parallel)
```bash
# File: examples/basic/array_copy.poly  
polyopt compile examples/basic/array_copy.poly --openmp
```

---

### Linear Algebra

#### Matrix Multiplication
```bash
# Analyze dependencies
polyopt analyze examples/linalg/matmul.poly --deps

# Generate tiled code
polyopt optimize examples/linalg/matmul.poly --tile 32 --openmp -o matmul_tiled.c

# Benchmark
polyopt bench examples/linalg/matmul.poly -N 512 --openmp
```

#### Matrix-Vector Multiply
```bash
# File: examples/linalg/matvec.poly
polyopt compile examples/linalg/matvec.poly --openmp
```

#### Matrix Transpose
```bash
# File: examples/linalg/transpose.poly
# Both loops parallelizable
polyopt analyze examples/linalg/transpose.poly --parallel
```

---

### Stencil Computations

#### Jacobi 2D (Double-Buffered - Parallel)
```bash
# File: examples/stencils/jacobi_2d.poly
# Reads from A, writes to B - no loop-carried dependencies
polyopt analyze examples/stencils/jacobi_2d.poly --parallel
polyopt compile examples/stencils/jacobi_2d.poly --openmp
```

#### Gauss-Seidel 2D (In-Place - Sequential)
```bash
# File: examples/stencils/gauss_seidel_2d.poly
# In-place update creates dependencies
polyopt analyze examples/stencils/gauss_seidel_2d.poly --deps
# Shows loop-carried dependencies - needs wavefront parallelization
```

#### 5-Point Stencil
```bash
# File: examples/stencils/stencil_2d_5pt.poly
polyvis examples/stencils/stencil_2d_5pt.poly --params N=10
```

---

### Reductions

#### Vector Sum
```bash
# File: examples/reductions/vector_sum.poly
polyopt compile examples/reductions/vector_sum.poly --openmp
# Generates: #pragma omp parallel for reduction(+:result)
```

#### Dot Product
```bash
# File: examples/reductions/dot_product.poly
polyopt compile examples/reductions/dot_product.poly --openmp
```

---

### Transformation Examples

#### Loop Interchange
```bash
# File: examples/transformations/interchange_example.poly
# Original has poor cache behavior (column-major access)
polyopt analyze examples/transformations/interchange_example.poly --deps
polyopt optimize examples/transformations/interchange_example.poly --schedule pluto
```

#### Loop Tiling
```bash
# File: examples/transformations/tiling_example.poly
# Matrix multiply benefits from tiling
polyopt optimize examples/transformations/tiling_example.poly --tile 32 -o tiled.c
```

#### Loop Fusion
```bash
# File: examples/transformations/fusion_example.poly
# Multiple loops over same iteration space
polyopt analyze examples/transformations/fusion_example.poly --deps
```

---

## Interactive Workflow Tutorial

### Step 1: Write Your Kernel

Create `mykernel.poly`:
```c
func saxpy(X[N], Y[N]) {
    for i = 0 to N {
        Y[i] = 2.0 * X[i] + Y[i];
    }
}
```

### Step 2: Analyze Dependencies

```bash
$ polyopt analyze mykernel.poly --deps --parallel

=== Analysis: saxpy ===

Statements: 1
Parameters: ["N"]
Arrays: ["X", "Y"]

--- Dependences ---
Total: 0
  Flow (RAW): 0
  Anti (WAR): 0
  Output (WAW): 0

--- Parallelism ---
  Level 0: ✓ PARALLEL

Recommendation: Parallelize at level 0
```

### Step 3: Visualize

```bash
$ polyvis mykernel.poly --params N=10

Statement S0 (1D domain):
  Iteration space (10 points):
    i→  0 1 2 3 4 5 6 7 8 9
        ● ● ● ● ● ● ● ● ● ●

Parallelism: Parallel at level 0
```

### Step 4: Generate Parallel Code

```bash
$ polyopt compile mykernel.poly --openmp

void saxpy(int N, double* restrict X, double* restrict Y) {
    // Statement S0: S0
    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        Y[i] = ((2.0 * X[i]) + Y[i]);
    }
}
```

### Step 5: Benchmark

```bash
$ polyopt bench mykernel.poly -N 10000000 --iterations 5 --openmp

=== Benchmark: saxpy ===
Size: N = 10000000
Iterations: 5
OpenMP: enabled

Running 5 iterations...

  Run 1: Time: 0.0234 seconds
  Run 2: Time: 0.0231 seconds
  Run 3: Time: 0.0229 seconds
  Run 4: Time: 0.0232 seconds
  Run 5: Time: 0.0230 seconds
```

---

## The Polyhedral Model

### What is the Polyhedral Model?

The polyhedral model represents loop nests as geometric objects:

- **Iteration Space**: The set of all loop iterations, represented as integer points in a polyhedron
- **Schedule**: A mapping that determines execution order
- **Access Functions**: Affine functions describing array accesses

### Why Polyhedral?

| Traditional Compiler | Polyhedral Compiler |
|---------------------|---------------------|
| Pattern matching | Mathematical reasoning |
| Limited scope | Global optimization |
| Heuristic | Exact dependence analysis |

### Example: Matrix Multiply

Original code:
```c
for (i = 0; i < N; i++)
  for (j = 0; j < N; j++)
    for (k = 0; k < N; k++)
      C[i][j] += A[i][k] * B[k][j];
```

Polyhedral representation:
- **Domain**: `{ S[i,j,k] : 0 ≤ i,j,k < N }`
- **Schedule**: `{ S[i,j,k] → [i,j,k] }` (original order)
- **Read accesses**: `{ S[i,j,k] → A[i,k] }`, `{ S[i,j,k] → B[k,j] }`, `{ S[i,j,k] → C[i,j] }`
- **Write access**: `{ S[i,j,k] → C[i,j] }`

### Dependence Types

| Type | Also Called | Pattern | Example |
|------|-------------|---------|---------|
| Flow | RAW (Read After Write) | Write → Read | `A[i] = ...; ... = A[i];` |
| Anti | WAR (Write After Read) | Read → Write | `... = A[i]; A[i] = ...;` |
| Output | WAW (Write After Write) | Write → Write | `A[i] = ...; A[i] = ...;` |

### Transformations

| Transformation | Effect | When to Use |
|---------------|--------|-------------|
| **Tiling** | Block loops for cache | Large working sets |
| **Interchange** | Swap loop order | Improve memory access pattern |
| **Fusion** | Merge loops | Improve temporal locality |
| **Distribution** | Split loops | Enable parallelization |
| **Skewing** | Add loop-carried values | Enable wavefront parallelism |

---

## Complete Example Listing

```
examples/
├── basic/
│   ├── vector_add.poly      # Simple parallel loop
│   ├── vector_scale.poly    # Scalar multiplication
│   ├── array_copy.poly      # 2D parallel copy
│   └── array_init.poly      # Index-dependent init
│
├── linalg/
│   ├── matmul.poly          # Matrix multiplication
│   ├── matvec.poly          # Matrix-vector multiply
│   ├── transpose.poly       # Matrix transpose
│   ├── trisolve.poly        # Triangular solve
│   ├── symm.poly            # Symmetric matrix multiply
│   ├── atax.poly            # A^T * A * x kernel
│   └── bicg.poly            # Biconjugate gradient
│
├── stencils/
│   ├── stencil_1d.poly      # 1D 3-point stencil
│   ├── stencil_2d_5pt.poly  # 2D 5-point stencil
│   ├── stencil_2d_9pt.poly  # 2D 9-point stencil
│   ├── jacobi_2d.poly       # Jacobi iteration (parallel)
│   ├── gauss_seidel_2d.poly # Gauss-Seidel (sequential)
│   └── conv2d.poly          # 2D convolution
│
├── reductions/
│   ├── vector_sum.poly      # Sum reduction
│   ├── dot_product.poly     # Dot product
│   ├── frobenius_norm.poly  # Matrix norm
│   └── prefix_sum.poly      # Prefix sum (sequential)
│
├── parallel/
│   ├── embarrassingly_parallel.poly  # Perfect parallelism
│   └── rowsum.poly          # Parallel outer, reduction inner
│
└── transformations/
    ├── interchange_example.poly   # Loop interchange demo
    ├── tiling_example.poly        # Tiling demo
    ├── fusion_example.poly        # Loop fusion demo
    └── distribution_example.poly  # Loop distribution demo
```

---

## Tips & Best Practices

### Writing Analyzable Code

1. **Use affine bounds**: `0 to N`, `1 to N-1`, not `0 to f(x)`
2. **Use affine indices**: `A[i][j]`, `A[i+1][j-1]`, not `A[B[i]]`
3. **Separate read/write arrays** when possible (enables more parallelism)
4. **Use compound assignment** for reductions: `sum = sum + x`

### Optimization Strategy

1. **Analyze first**: Understand dependencies before optimizing
2. **Check parallelism**: Use `--parallel` to find parallel loops
3. **Consider tiling**: Large iteration spaces benefit from tiling
4. **Benchmark**: Always verify optimizations improve performance

### Common Patterns

| Pattern | Parallelizable? | Optimization |
|---------|----------------|--------------|
| Element-wise ops | ✓ Yes | OpenMP parallel for |
| Stencil (double-buffer) | ✓ Yes | Tiling + OpenMP |
| Stencil (in-place) | ✗ No | Wavefront/skewing |
| Reduction | Partial | OpenMP reduction clause |
| Sequential scan | ✗ No | Algorithm change needed |

---

## Troubleshooting

### "Unknown variable in affine expression"

The variable wasn't declared as a loop iterator or parameter.

```c
// Wrong - T not defined
for i = 0 to T { ... }

// Right - T is a parameter
func foo(A[T]) {
    for i = 0 to T { ... }
}
```

### "No parallel loops found"

Check for loop-carried dependencies:
```bash
polyopt analyze input.poly --deps
```

Common causes:
- In-place array updates: `A[i] = f(A[i-1])`
- Reduction without proper pattern
- Complex index expressions

### Generated code doesn't compile

Ensure indices match array dimensions:
```c
// Wrong - A is 1D but accessed as 2D
func foo(A[N]) {
    for i = 0 to N {
        A[i][j] = 0;  // Error!
    }
}
```

---

## Contributing

Contributions welcome! Areas of interest:
- Additional scheduling algorithms
- GPU code generation (CUDA/OpenCL)
- More transformation passes
- Performance benchmarking

---

## License

MIT License - see LICENSE file

---

## Acknowledgments

PolyOpt is inspired by:
- [ISL](https://libisl.sourceforge.io/) - Integer Set Library
- [Pluto](http://pluto-compiler.sourceforge.net/) - Automatic parallelizer
- [Polly](https://polly.llvm.org/) - LLVM polyhedral optimizer
- [PolyBench](http://web.cse.ohio-state.edu/~pouchet.2/software/polybench/) - Polyhedral benchmark suite

---

<p align="center">
  Made with ❤️ for the HPC community
</p>
